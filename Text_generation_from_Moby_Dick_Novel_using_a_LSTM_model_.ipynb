{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation from Moby-Dick Novel using a LSTM model .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb+DIj/g5wMvsBtyrTZAdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulya-Anurag/Text-generation-from-Moby-Dick-Novel-using-a-LSTM-model/blob/master/Text_generation_from_Moby_Dick_Novel_using_a_LSTM_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Fc_rI0D_eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevent libraries\n",
        "import numpy as np\n",
        "import spacy\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBBhmn9LEDhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a functions to read file\n",
        "def read_txt(path):\n",
        "  with open(path) as file:\n",
        "    text= file.read()\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tENadd8ifhVc",
        "colab_type": "text"
      },
      "source": [
        "Here, Only 4 chapters of book has been used due to limited RAM availability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19IGU-EXEJ5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "a394179f-6775-4a4a-db72-2bf49db9084c"
      },
      "source": [
        "# Directory of Book\n",
        "path='/content/sample_data/moby_dick_four_chapters.txt'\n",
        "txt=read_txt(path)\n",
        "# Visualize the text\n",
        "print(txt[:500])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call me Ishmael.  Some years ago--never mind how long\n",
            "precisely--having little or no money in my purse, and nothing\n",
            "particular to interest me on shore, I thought I would sail about a\n",
            "little and see the watery part of the world.  It is a way I have of\n",
            "driving off the spleen and regulating the circulation.  Whenever I\n",
            "find myself growing grim about the mouth; whenever it is a damp,\n",
            "drizzly November in my soul; whenever I find myself involuntarily\n",
            "pausing before coffin warehouses, and bringing up t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbuW9fyArsea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import english library from SPACY and disable ner, parser, and tagger for more speed\n",
        "nlp=spacy.load('en',disable=['ner','parser','tagger'])\n",
        "\n",
        "# define the max length to avoid any issue in future\n",
        "nlp.max_length=2000000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7LSyFJvgkXO",
        "colab_type": "text"
      },
      "source": [
        "Create a function to remove Puncuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4HeCzzDsJ0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punc(txt):\n",
        "  return [token.text for token in nlp(txt) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n '] # These punctuation has been taken from Keras module and by observing the text file"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijr0GoyNtWlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "429c21d7-5f45-464f-df17-ebaf19187aed"
      },
      "source": [
        "# Remove Punctuation from text\n",
        "tokens =remove_punc(txt)\n",
        "len(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSUTlUaOuRIe",
        "colab_type": "text"
      },
      "source": [
        "Lets create a training sequence of 30 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ZFJjgTtkus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define training sequence\n",
        "train_len = 30 +1\n",
        "xtrain_sentence=[ ]\n",
        "\n",
        "for i in range(train_len,len(tokens)):\n",
        "   xtrain_sentence.append(tokens[i-train_len : i])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbaH3jzuhIWr",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the training Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtpo3AhHCsaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tokenizer from tensorflow and convert text into sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer= Tokenizer()\n",
        "tokenizer.fit_on_texts(xtrain_sentence)\n",
        "train_seq= tokenizer.texts_to_sequences(xtrain_sentence)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jITpv9vShXMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "b5c3e455-702b-460c-f4d4-45ddc69e80df"
      },
      "source": [
        "# Take a peek at vocab\n",
        "list(tokenizer.word_index.items())[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 1),\n",
              " ('a', 2),\n",
              " ('and', 3),\n",
              " ('of', 4),\n",
              " ('i', 5),\n",
              " ('to', 6),\n",
              " ('in', 7),\n",
              " ('it', 8),\n",
              " ('that', 9),\n",
              " ('he', 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0tsrGchDjHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocab size will also include zeroth position so 1 is added here\n",
        "vocab_size=len(tokenizer.word_counts)+1\n",
        "\n",
        "# convert training seq into numpy array for smooth functioning\n",
        "train_seq=np.array(train_seq)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuFwTR7wIBd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into input and targets\n",
        "xtrain=train_seq[:,:-1]\n",
        "ytrain=train_seq[:,-1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NTQqX_djNRj",
        "colab_type": "text"
      },
      "source": [
        "# Create LSTM model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07hJ9SnfLytJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevent libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM ,Embedding"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCYDNSn0MxQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9dae28cb-bbc8-42fb-d6b2-44fdf6817ca3"
      },
      "source": [
        "# Create class of ytrain \n",
        "ytrain=to_categorical(ytrain,num_classes=vocab_size)\n",
        "\n",
        "# Check shape of xtrain \n",
        "xtrain.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11307, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJqUjIT-PtX2",
        "colab_type": "text"
      },
      "source": [
        "LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzz2GEQEPurt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "d64732a3-6120-4a8a-91ee-07d342facdd9"
      },
      "source": [
        "# Model will include one Embedding layer, 2 LSTM layers, 2 Dense Layers\n",
        "model= Sequential([ \n",
        "                \n",
        "                  Embedding( input_dim= vocab_size, output_dim= 100, input_length=xtrain.shape[1] ),\n",
        "                   \n",
        "                  LSTM(units=100,return_sequences= True),\n",
        "                  LSTM(units=100),\n",
        "                   \n",
        "                  Dense(units=100,activation='relu'),\n",
        "                  Dense(units=vocab_size, activation='softmax')                   \n",
        "])\n",
        "\n",
        "# use adam optimizer and loss function would be categorical crossentropy\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "#Summary of Model\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 100)           271800    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 30, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2718)              274518    \n",
            "=================================================================\n",
            "Total params: 717,218\n",
            "Trainable params: 717,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slipHOM_j-XK",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c3xaY6eTRiv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c885bce-f81c-4de9-86a2-e908591473ea"
      },
      "source": [
        "model.fit(xtrain,ytrain,batch_size=128,epochs=300,verbose=2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "89/89 - 1s - loss: 6.8774 - accuracy: 0.0502\n",
            "Epoch 2/300\n",
            "89/89 - 1s - loss: 6.3807 - accuracy: 0.0529\n",
            "Epoch 3/300\n",
            "89/89 - 1s - loss: 6.2727 - accuracy: 0.0529\n",
            "Epoch 4/300\n",
            "89/89 - 1s - loss: 6.1444 - accuracy: 0.0529\n",
            "Epoch 5/300\n",
            "89/89 - 1s - loss: 6.0326 - accuracy: 0.0532\n",
            "Epoch 6/300\n",
            "89/89 - 1s - loss: 5.9224 - accuracy: 0.0626\n",
            "Epoch 7/300\n",
            "89/89 - 1s - loss: 5.8340 - accuracy: 0.0656\n",
            "Epoch 8/300\n",
            "89/89 - 1s - loss: 5.7558 - accuracy: 0.0685\n",
            "Epoch 9/300\n",
            "89/89 - 1s - loss: 5.6815 - accuracy: 0.0705\n",
            "Epoch 10/300\n",
            "89/89 - 1s - loss: 5.6191 - accuracy: 0.0740\n",
            "Epoch 11/300\n",
            "89/89 - 1s - loss: 5.5578 - accuracy: 0.0768\n",
            "Epoch 12/300\n",
            "89/89 - 1s - loss: 5.5011 - accuracy: 0.0769\n",
            "Epoch 13/300\n",
            "89/89 - 1s - loss: 5.4374 - accuracy: 0.0805\n",
            "Epoch 14/300\n",
            "89/89 - 1s - loss: 5.3733 - accuracy: 0.0821\n",
            "Epoch 15/300\n",
            "89/89 - 1s - loss: 5.3062 - accuracy: 0.0847\n",
            "Epoch 16/300\n",
            "89/89 - 1s - loss: 5.2383 - accuracy: 0.0907\n",
            "Epoch 17/300\n",
            "89/89 - 1s - loss: 5.1680 - accuracy: 0.0938\n",
            "Epoch 18/300\n",
            "89/89 - 1s - loss: 5.1002 - accuracy: 0.0998\n",
            "Epoch 19/300\n",
            "89/89 - 1s - loss: 5.0385 - accuracy: 0.1086\n",
            "Epoch 20/300\n",
            "89/89 - 1s - loss: 4.9766 - accuracy: 0.1100\n",
            "Epoch 21/300\n",
            "89/89 - 1s - loss: 4.9180 - accuracy: 0.1194\n",
            "Epoch 22/300\n",
            "89/89 - 1s - loss: 4.8671 - accuracy: 0.1204\n",
            "Epoch 23/300\n",
            "89/89 - 1s - loss: 4.8126 - accuracy: 0.1236\n",
            "Epoch 24/300\n",
            "89/89 - 1s - loss: 4.7528 - accuracy: 0.1236\n",
            "Epoch 25/300\n",
            "89/89 - 1s - loss: 4.6966 - accuracy: 0.1323\n",
            "Epoch 26/300\n",
            "89/89 - 1s - loss: 4.6395 - accuracy: 0.1320\n",
            "Epoch 27/300\n",
            "89/89 - 1s - loss: 4.5851 - accuracy: 0.1331\n",
            "Epoch 28/300\n",
            "89/89 - 1s - loss: 4.5300 - accuracy: 0.1409\n",
            "Epoch 29/300\n",
            "89/89 - 1s - loss: 4.4857 - accuracy: 0.1391\n",
            "Epoch 30/300\n",
            "89/89 - 1s - loss: 4.4288 - accuracy: 0.1450\n",
            "Epoch 31/300\n",
            "89/89 - 1s - loss: 4.3816 - accuracy: 0.1438\n",
            "Epoch 32/300\n",
            "89/89 - 1s - loss: 4.3343 - accuracy: 0.1486\n",
            "Epoch 33/300\n",
            "89/89 - 1s - loss: 4.2809 - accuracy: 0.1522\n",
            "Epoch 34/300\n",
            "89/89 - 1s - loss: 4.2325 - accuracy: 0.1547\n",
            "Epoch 35/300\n",
            "89/89 - 1s - loss: 4.1896 - accuracy: 0.1590\n",
            "Epoch 36/300\n",
            "89/89 - 1s - loss: 4.1510 - accuracy: 0.1614\n",
            "Epoch 37/300\n",
            "89/89 - 1s - loss: 4.1018 - accuracy: 0.1659\n",
            "Epoch 38/300\n",
            "89/89 - 1s - loss: 4.0628 - accuracy: 0.1656\n",
            "Epoch 39/300\n",
            "89/89 - 1s - loss: 4.0152 - accuracy: 0.1727\n",
            "Epoch 40/300\n",
            "89/89 - 1s - loss: 3.9784 - accuracy: 0.1732\n",
            "Epoch 41/300\n",
            "89/89 - 1s - loss: 3.9368 - accuracy: 0.1784\n",
            "Epoch 42/300\n",
            "89/89 - 1s - loss: 3.8951 - accuracy: 0.1819\n",
            "Epoch 43/300\n",
            "89/89 - 1s - loss: 3.8513 - accuracy: 0.1829\n",
            "Epoch 44/300\n",
            "89/89 - 1s - loss: 3.8125 - accuracy: 0.1867\n",
            "Epoch 45/300\n",
            "89/89 - 1s - loss: 3.7760 - accuracy: 0.1919\n",
            "Epoch 46/300\n",
            "89/89 - 1s - loss: 3.7373 - accuracy: 0.1948\n",
            "Epoch 47/300\n",
            "89/89 - 1s - loss: 3.6949 - accuracy: 0.1992\n",
            "Epoch 48/300\n",
            "89/89 - 1s - loss: 3.6556 - accuracy: 0.2045\n",
            "Epoch 49/300\n",
            "89/89 - 1s - loss: 3.6163 - accuracy: 0.2102\n",
            "Epoch 50/300\n",
            "89/89 - 1s - loss: 3.5829 - accuracy: 0.2085\n",
            "Epoch 51/300\n",
            "89/89 - 1s - loss: 3.5535 - accuracy: 0.2152\n",
            "Epoch 52/300\n",
            "89/89 - 1s - loss: 3.5061 - accuracy: 0.2267\n",
            "Epoch 53/300\n",
            "89/89 - 1s - loss: 3.4729 - accuracy: 0.2259\n",
            "Epoch 54/300\n",
            "89/89 - 1s - loss: 3.4365 - accuracy: 0.2321\n",
            "Epoch 55/300\n",
            "89/89 - 1s - loss: 3.4012 - accuracy: 0.2328\n",
            "Epoch 56/300\n",
            "89/89 - 1s - loss: 3.3625 - accuracy: 0.2442\n",
            "Epoch 57/300\n",
            "89/89 - 1s - loss: 3.3450 - accuracy: 0.2444\n",
            "Epoch 58/300\n",
            "89/89 - 1s - loss: 3.2972 - accuracy: 0.2528\n",
            "Epoch 59/300\n",
            "89/89 - 1s - loss: 3.2648 - accuracy: 0.2561\n",
            "Epoch 60/300\n",
            "89/89 - 1s - loss: 3.2286 - accuracy: 0.2674\n",
            "Epoch 61/300\n",
            "89/89 - 1s - loss: 3.1937 - accuracy: 0.2692\n",
            "Epoch 62/300\n",
            "89/89 - 1s - loss: 3.1547 - accuracy: 0.2746\n",
            "Epoch 63/300\n",
            "89/89 - 1s - loss: 3.1328 - accuracy: 0.2792\n",
            "Epoch 64/300\n",
            "89/89 - 1s - loss: 3.0937 - accuracy: 0.2838\n",
            "Epoch 65/300\n",
            "89/89 - 1s - loss: 3.0484 - accuracy: 0.2923\n",
            "Epoch 66/300\n",
            "89/89 - 1s - loss: 3.0243 - accuracy: 0.3013\n",
            "Epoch 67/300\n",
            "89/89 - 1s - loss: 2.9843 - accuracy: 0.3086\n",
            "Epoch 68/300\n",
            "89/89 - 1s - loss: 2.9516 - accuracy: 0.3144\n",
            "Epoch 69/300\n",
            "89/89 - 1s - loss: 2.9191 - accuracy: 0.3173\n",
            "Epoch 70/300\n",
            "89/89 - 1s - loss: 2.8853 - accuracy: 0.3308\n",
            "Epoch 71/300\n",
            "89/89 - 1s - loss: 2.8595 - accuracy: 0.3273\n",
            "Epoch 72/300\n",
            "89/89 - 1s - loss: 2.8213 - accuracy: 0.3430\n",
            "Epoch 73/300\n",
            "89/89 - 1s - loss: 2.7943 - accuracy: 0.3399\n",
            "Epoch 74/300\n",
            "89/89 - 1s - loss: 2.7595 - accuracy: 0.3471\n",
            "Epoch 75/300\n",
            "89/89 - 1s - loss: 2.7278 - accuracy: 0.3593\n",
            "Epoch 76/300\n",
            "89/89 - 1s - loss: 2.6921 - accuracy: 0.3660\n",
            "Epoch 77/300\n",
            "89/89 - 1s - loss: 2.6644 - accuracy: 0.3669\n",
            "Epoch 78/300\n",
            "89/89 - 1s - loss: 2.6318 - accuracy: 0.3791\n",
            "Epoch 79/300\n",
            "89/89 - 1s - loss: 2.6068 - accuracy: 0.3830\n",
            "Epoch 80/300\n",
            "89/89 - 1s - loss: 2.5684 - accuracy: 0.3882\n",
            "Epoch 81/300\n",
            "89/89 - 1s - loss: 2.5474 - accuracy: 0.3914\n",
            "Epoch 82/300\n",
            "89/89 - 1s - loss: 2.5158 - accuracy: 0.4021\n",
            "Epoch 83/300\n",
            "89/89 - 1s - loss: 2.4933 - accuracy: 0.4015\n",
            "Epoch 84/300\n",
            "89/89 - 1s - loss: 2.4549 - accuracy: 0.4127\n",
            "Epoch 85/300\n",
            "89/89 - 1s - loss: 2.4341 - accuracy: 0.4193\n",
            "Epoch 86/300\n",
            "89/89 - 1s - loss: 2.3982 - accuracy: 0.4268\n",
            "Epoch 87/300\n",
            "89/89 - 1s - loss: 2.3679 - accuracy: 0.4289\n",
            "Epoch 88/300\n",
            "89/89 - 1s - loss: 2.3471 - accuracy: 0.4410\n",
            "Epoch 89/300\n",
            "89/89 - 1s - loss: 2.3136 - accuracy: 0.4444\n",
            "Epoch 90/300\n",
            "89/89 - 1s - loss: 2.2938 - accuracy: 0.4457\n",
            "Epoch 91/300\n",
            "89/89 - 1s - loss: 2.2596 - accuracy: 0.4537\n",
            "Epoch 92/300\n",
            "89/89 - 1s - loss: 2.2453 - accuracy: 0.4579\n",
            "Epoch 93/300\n",
            "89/89 - 1s - loss: 2.2169 - accuracy: 0.4634\n",
            "Epoch 94/300\n",
            "89/89 - 1s - loss: 2.1936 - accuracy: 0.4698\n",
            "Epoch 95/300\n",
            "89/89 - 1s - loss: 2.1689 - accuracy: 0.4773\n",
            "Epoch 96/300\n",
            "89/89 - 1s - loss: 2.1521 - accuracy: 0.4751\n",
            "Epoch 97/300\n",
            "89/89 - 1s - loss: 2.1148 - accuracy: 0.4899\n",
            "Epoch 98/300\n",
            "89/89 - 1s - loss: 2.0940 - accuracy: 0.4924\n",
            "Epoch 99/300\n",
            "89/89 - 1s - loss: 2.0844 - accuracy: 0.4905\n",
            "Epoch 100/300\n",
            "89/89 - 1s - loss: 2.0540 - accuracy: 0.4951\n",
            "Epoch 101/300\n",
            "89/89 - 1s - loss: 2.0240 - accuracy: 0.5122\n",
            "Epoch 102/300\n",
            "89/89 - 1s - loss: 2.0056 - accuracy: 0.5135\n",
            "Epoch 103/300\n",
            "89/89 - 1s - loss: 1.9883 - accuracy: 0.5144\n",
            "Epoch 104/300\n",
            "89/89 - 1s - loss: 1.9644 - accuracy: 0.5209\n",
            "Epoch 105/300\n",
            "89/89 - 1s - loss: 1.9383 - accuracy: 0.5249\n",
            "Epoch 106/300\n",
            "89/89 - 1s - loss: 1.9217 - accuracy: 0.5314\n",
            "Epoch 107/300\n",
            "89/89 - 1s - loss: 1.9006 - accuracy: 0.5337\n",
            "Epoch 108/300\n",
            "89/89 - 1s - loss: 1.8980 - accuracy: 0.5366\n",
            "Epoch 109/300\n",
            "89/89 - 1s - loss: 1.8740 - accuracy: 0.5378\n",
            "Epoch 110/300\n",
            "89/89 - 1s - loss: 1.8494 - accuracy: 0.5428\n",
            "Epoch 111/300\n",
            "89/89 - 1s - loss: 1.8239 - accuracy: 0.5529\n",
            "Epoch 112/300\n",
            "89/89 - 1s - loss: 1.8065 - accuracy: 0.5573\n",
            "Epoch 113/300\n",
            "89/89 - 1s - loss: 1.7855 - accuracy: 0.5637\n",
            "Epoch 114/300\n",
            "89/89 - 1s - loss: 1.7765 - accuracy: 0.5627\n",
            "Epoch 115/300\n",
            "89/89 - 1s - loss: 1.7494 - accuracy: 0.5664\n",
            "Epoch 116/300\n",
            "89/89 - 1s - loss: 1.7326 - accuracy: 0.5724\n",
            "Epoch 117/300\n",
            "89/89 - 1s - loss: 1.7137 - accuracy: 0.5779\n",
            "Epoch 118/300\n",
            "89/89 - 1s - loss: 1.6991 - accuracy: 0.5793\n",
            "Epoch 119/300\n",
            "89/89 - 1s - loss: 1.6824 - accuracy: 0.5879\n",
            "Epoch 120/300\n",
            "89/89 - 1s - loss: 1.6607 - accuracy: 0.5852\n",
            "Epoch 121/300\n",
            "89/89 - 1s - loss: 1.6430 - accuracy: 0.5926\n",
            "Epoch 122/300\n",
            "89/89 - 1s - loss: 1.6296 - accuracy: 0.6001\n",
            "Epoch 123/300\n",
            "89/89 - 1s - loss: 1.6135 - accuracy: 0.6006\n",
            "Epoch 124/300\n",
            "89/89 - 1s - loss: 1.5938 - accuracy: 0.6078\n",
            "Epoch 125/300\n",
            "89/89 - 1s - loss: 1.5755 - accuracy: 0.6079\n",
            "Epoch 126/300\n",
            "89/89 - 1s - loss: 1.5689 - accuracy: 0.6127\n",
            "Epoch 127/300\n",
            "89/89 - 1s - loss: 1.5596 - accuracy: 0.6110\n",
            "Epoch 128/300\n",
            "89/89 - 1s - loss: 1.5488 - accuracy: 0.6149\n",
            "Epoch 129/300\n",
            "89/89 - 1s - loss: 1.5326 - accuracy: 0.6218\n",
            "Epoch 130/300\n",
            "89/89 - 1s - loss: 1.5091 - accuracy: 0.6256\n",
            "Epoch 131/300\n",
            "89/89 - 1s - loss: 1.4932 - accuracy: 0.6328\n",
            "Epoch 132/300\n",
            "89/89 - 1s - loss: 1.4693 - accuracy: 0.6318\n",
            "Epoch 133/300\n",
            "89/89 - 1s - loss: 1.4702 - accuracy: 0.6354\n",
            "Epoch 134/300\n",
            "89/89 - 1s - loss: 1.4519 - accuracy: 0.6405\n",
            "Epoch 135/300\n",
            "89/89 - 1s - loss: 1.4274 - accuracy: 0.6457\n",
            "Epoch 136/300\n",
            "89/89 - 1s - loss: 1.4148 - accuracy: 0.6509\n",
            "Epoch 137/300\n",
            "89/89 - 1s - loss: 1.4144 - accuracy: 0.6461\n",
            "Epoch 138/300\n",
            "89/89 - 1s - loss: 1.4008 - accuracy: 0.6489\n",
            "Epoch 139/300\n",
            "89/89 - 1s - loss: 1.3826 - accuracy: 0.6594\n",
            "Epoch 140/300\n",
            "89/89 - 1s - loss: 1.3608 - accuracy: 0.6605\n",
            "Epoch 141/300\n",
            "89/89 - 1s - loss: 1.3516 - accuracy: 0.6610\n",
            "Epoch 142/300\n",
            "89/89 - 1s - loss: 1.3393 - accuracy: 0.6683\n",
            "Epoch 143/300\n",
            "89/89 - 1s - loss: 1.3267 - accuracy: 0.6664\n",
            "Epoch 144/300\n",
            "89/89 - 1s - loss: 1.3188 - accuracy: 0.6698\n",
            "Epoch 145/300\n",
            "89/89 - 1s - loss: 1.2941 - accuracy: 0.6745\n",
            "Epoch 146/300\n",
            "89/89 - 1s - loss: 1.2935 - accuracy: 0.6773\n",
            "Epoch 147/300\n",
            "89/89 - 1s - loss: 1.2783 - accuracy: 0.6785\n",
            "Epoch 148/300\n",
            "89/89 - 1s - loss: 1.2579 - accuracy: 0.6884\n",
            "Epoch 149/300\n",
            "89/89 - 1s - loss: 1.2412 - accuracy: 0.6916\n",
            "Epoch 150/300\n",
            "89/89 - 1s - loss: 1.2320 - accuracy: 0.6924\n",
            "Epoch 151/300\n",
            "89/89 - 1s - loss: 1.2290 - accuracy: 0.6910\n",
            "Epoch 152/300\n",
            "89/89 - 1s - loss: 1.2115 - accuracy: 0.6954\n",
            "Epoch 153/300\n",
            "89/89 - 1s - loss: 1.1940 - accuracy: 0.6997\n",
            "Epoch 154/300\n",
            "89/89 - 1s - loss: 1.1736 - accuracy: 0.7063\n",
            "Epoch 155/300\n",
            "89/89 - 1s - loss: 1.1595 - accuracy: 0.7138\n",
            "Epoch 156/300\n",
            "89/89 - 1s - loss: 1.1556 - accuracy: 0.7118\n",
            "Epoch 157/300\n",
            "89/89 - 1s - loss: 1.1615 - accuracy: 0.7094\n",
            "Epoch 158/300\n",
            "89/89 - 1s - loss: 1.1404 - accuracy: 0.7150\n",
            "Epoch 159/300\n",
            "89/89 - 1s - loss: 1.1295 - accuracy: 0.7180\n",
            "Epoch 160/300\n",
            "89/89 - 1s - loss: 1.1388 - accuracy: 0.7132\n",
            "Epoch 161/300\n",
            "89/89 - 1s - loss: 1.1153 - accuracy: 0.7165\n",
            "Epoch 162/300\n",
            "89/89 - 1s - loss: 1.1029 - accuracy: 0.7209\n",
            "Epoch 163/300\n",
            "89/89 - 1s - loss: 1.0895 - accuracy: 0.7239\n",
            "Epoch 164/300\n",
            "89/89 - 1s - loss: 1.0778 - accuracy: 0.7252\n",
            "Epoch 165/300\n",
            "89/89 - 1s - loss: 1.0548 - accuracy: 0.7381\n",
            "Epoch 166/300\n",
            "89/89 - 1s - loss: 1.0506 - accuracy: 0.7350\n",
            "Epoch 167/300\n",
            "89/89 - 1s - loss: 1.0525 - accuracy: 0.7313\n",
            "Epoch 168/300\n",
            "89/89 - 1s - loss: 1.0339 - accuracy: 0.7408\n",
            "Epoch 169/300\n",
            "89/89 - 1s - loss: 1.0153 - accuracy: 0.7441\n",
            "Epoch 170/300\n",
            "89/89 - 1s - loss: 1.0097 - accuracy: 0.7509\n",
            "Epoch 171/300\n",
            "89/89 - 1s - loss: 1.0029 - accuracy: 0.7452\n",
            "Epoch 172/300\n",
            "89/89 - 1s - loss: 0.9914 - accuracy: 0.7519\n",
            "Epoch 173/300\n",
            "89/89 - 1s - loss: 0.9740 - accuracy: 0.7548\n",
            "Epoch 174/300\n",
            "89/89 - 1s - loss: 0.9615 - accuracy: 0.7579\n",
            "Epoch 175/300\n",
            "89/89 - 1s - loss: 0.9521 - accuracy: 0.7602\n",
            "Epoch 176/300\n",
            "89/89 - 1s - loss: 0.9591 - accuracy: 0.7591\n",
            "Epoch 177/300\n",
            "89/89 - 1s - loss: 0.9357 - accuracy: 0.7649\n",
            "Epoch 178/300\n",
            "89/89 - 1s - loss: 0.9356 - accuracy: 0.7663\n",
            "Epoch 179/300\n",
            "89/89 - 1s - loss: 0.9372 - accuracy: 0.7638\n",
            "Epoch 180/300\n",
            "89/89 - 1s - loss: 0.9408 - accuracy: 0.7598\n",
            "Epoch 181/300\n",
            "89/89 - 1s - loss: 0.9186 - accuracy: 0.7674\n",
            "Epoch 182/300\n",
            "89/89 - 1s - loss: 0.9014 - accuracy: 0.7739\n",
            "Epoch 183/300\n",
            "89/89 - 1s - loss: 0.8793 - accuracy: 0.7779\n",
            "Epoch 184/300\n",
            "89/89 - 1s - loss: 0.8686 - accuracy: 0.7832\n",
            "Epoch 185/300\n",
            "89/89 - 1s - loss: 0.8646 - accuracy: 0.7819\n",
            "Epoch 186/300\n",
            "89/89 - 1s - loss: 0.8551 - accuracy: 0.7873\n",
            "Epoch 187/300\n",
            "89/89 - 1s - loss: 0.8402 - accuracy: 0.7897\n",
            "Epoch 188/300\n",
            "89/89 - 1s - loss: 0.8313 - accuracy: 0.7924\n",
            "Epoch 189/300\n",
            "89/89 - 1s - loss: 0.8237 - accuracy: 0.7966\n",
            "Epoch 190/300\n",
            "89/89 - 1s - loss: 0.8341 - accuracy: 0.7943\n",
            "Epoch 191/300\n",
            "89/89 - 1s - loss: 0.8125 - accuracy: 0.7956\n",
            "Epoch 192/300\n",
            "89/89 - 1s - loss: 0.8068 - accuracy: 0.7970\n",
            "Epoch 193/300\n",
            "89/89 - 1s - loss: 0.7895 - accuracy: 0.8046\n",
            "Epoch 194/300\n",
            "89/89 - 1s - loss: 0.7867 - accuracy: 0.8038\n",
            "Epoch 195/300\n",
            "89/89 - 1s - loss: 0.7777 - accuracy: 0.8072\n",
            "Epoch 196/300\n",
            "89/89 - 1s - loss: 0.7744 - accuracy: 0.8060\n",
            "Epoch 197/300\n",
            "89/89 - 1s - loss: 0.7999 - accuracy: 0.7946\n",
            "Epoch 198/300\n",
            "89/89 - 1s - loss: 0.7877 - accuracy: 0.8040\n",
            "Epoch 199/300\n",
            "89/89 - 1s - loss: 0.7503 - accuracy: 0.8121\n",
            "Epoch 200/300\n",
            "89/89 - 1s - loss: 0.7299 - accuracy: 0.8201\n",
            "Epoch 201/300\n",
            "89/89 - 1s - loss: 0.7232 - accuracy: 0.8219\n",
            "Epoch 202/300\n",
            "89/89 - 1s - loss: 0.7254 - accuracy: 0.8171\n",
            "Epoch 203/300\n",
            "89/89 - 1s - loss: 0.7108 - accuracy: 0.8238\n",
            "Epoch 204/300\n",
            "89/89 - 1s - loss: 0.6889 - accuracy: 0.8317\n",
            "Epoch 205/300\n",
            "89/89 - 1s - loss: 0.6879 - accuracy: 0.8340\n",
            "Epoch 206/300\n",
            "89/89 - 1s - loss: 0.6801 - accuracy: 0.8355\n",
            "Epoch 207/300\n",
            "89/89 - 1s - loss: 0.6950 - accuracy: 0.8237\n",
            "Epoch 208/300\n",
            "89/89 - 1s - loss: 0.6768 - accuracy: 0.8313\n",
            "Epoch 209/300\n",
            "89/89 - 1s - loss: 0.6719 - accuracy: 0.8349\n",
            "Epoch 210/300\n",
            "89/89 - 1s - loss: 0.6731 - accuracy: 0.8313\n",
            "Epoch 211/300\n",
            "89/89 - 1s - loss: 0.6438 - accuracy: 0.8436\n",
            "Epoch 212/300\n",
            "89/89 - 1s - loss: 0.6352 - accuracy: 0.8448\n",
            "Epoch 213/300\n",
            "89/89 - 1s - loss: 0.6199 - accuracy: 0.8487\n",
            "Epoch 214/300\n",
            "89/89 - 1s - loss: 0.6206 - accuracy: 0.8466\n",
            "Epoch 215/300\n",
            "89/89 - 1s - loss: 0.6170 - accuracy: 0.8489\n",
            "Epoch 216/300\n",
            "89/89 - 1s - loss: 0.6123 - accuracy: 0.8485\n",
            "Epoch 217/300\n",
            "89/89 - 1s - loss: 0.6054 - accuracy: 0.8489\n",
            "Epoch 218/300\n",
            "89/89 - 1s - loss: 0.6076 - accuracy: 0.8479\n",
            "Epoch 219/300\n",
            "89/89 - 1s - loss: 0.6070 - accuracy: 0.8497\n",
            "Epoch 220/300\n",
            "89/89 - 1s - loss: 0.5924 - accuracy: 0.8557\n",
            "Epoch 221/300\n",
            "89/89 - 1s - loss: 0.5700 - accuracy: 0.8654\n",
            "Epoch 222/300\n",
            "89/89 - 1s - loss: 0.5632 - accuracy: 0.8621\n",
            "Epoch 223/300\n",
            "89/89 - 1s - loss: 0.5606 - accuracy: 0.8632\n",
            "Epoch 224/300\n",
            "89/89 - 1s - loss: 0.5548 - accuracy: 0.8645\n",
            "Epoch 225/300\n",
            "89/89 - 1s - loss: 0.5569 - accuracy: 0.8642\n",
            "Epoch 226/300\n",
            "89/89 - 1s - loss: 0.5656 - accuracy: 0.8584\n",
            "Epoch 227/300\n",
            "89/89 - 1s - loss: 0.5576 - accuracy: 0.8652\n",
            "Epoch 228/300\n",
            "89/89 - 1s - loss: 0.5565 - accuracy: 0.8624\n",
            "Epoch 229/300\n",
            "89/89 - 1s - loss: 0.5442 - accuracy: 0.8644\n",
            "Epoch 230/300\n",
            "89/89 - 1s - loss: 0.5339 - accuracy: 0.8694\n",
            "Epoch 231/300\n",
            "89/89 - 1s - loss: 0.5061 - accuracy: 0.8767\n",
            "Epoch 232/300\n",
            "89/89 - 1s - loss: 0.4902 - accuracy: 0.8847\n",
            "Epoch 233/300\n",
            "89/89 - 1s - loss: 0.4849 - accuracy: 0.8839\n",
            "Epoch 234/300\n",
            "89/89 - 1s - loss: 0.4925 - accuracy: 0.8807\n",
            "Epoch 235/300\n",
            "89/89 - 1s - loss: 0.4768 - accuracy: 0.8903\n",
            "Epoch 236/300\n",
            "89/89 - 1s - loss: 0.4740 - accuracy: 0.8856\n",
            "Epoch 237/300\n",
            "89/89 - 1s - loss: 0.4710 - accuracy: 0.8896\n",
            "Epoch 238/300\n",
            "89/89 - 1s - loss: 0.4570 - accuracy: 0.8916\n",
            "Epoch 239/300\n",
            "89/89 - 1s - loss: 0.4694 - accuracy: 0.8855\n",
            "Epoch 240/300\n",
            "89/89 - 1s - loss: 0.4697 - accuracy: 0.8849\n",
            "Epoch 241/300\n",
            "89/89 - 1s - loss: 0.4717 - accuracy: 0.8871\n",
            "Epoch 242/300\n",
            "89/89 - 1s - loss: 0.4445 - accuracy: 0.8966\n",
            "Epoch 243/300\n",
            "89/89 - 1s - loss: 0.4472 - accuracy: 0.8939\n",
            "Epoch 244/300\n",
            "89/89 - 1s - loss: 0.4376 - accuracy: 0.8967\n",
            "Epoch 245/300\n",
            "89/89 - 1s - loss: 0.4222 - accuracy: 0.9025\n",
            "Epoch 246/300\n",
            "89/89 - 1s - loss: 0.4268 - accuracy: 0.9007\n",
            "Epoch 247/300\n",
            "89/89 - 1s - loss: 0.4282 - accuracy: 0.8971\n",
            "Epoch 248/300\n",
            "89/89 - 1s - loss: 0.4134 - accuracy: 0.8996\n",
            "Epoch 249/300\n",
            "89/89 - 1s - loss: 0.4090 - accuracy: 0.9063\n",
            "Epoch 250/300\n",
            "89/89 - 1s - loss: 0.3884 - accuracy: 0.9104\n",
            "Epoch 251/300\n",
            "89/89 - 1s - loss: 0.3785 - accuracy: 0.9144\n",
            "Epoch 252/300\n",
            "89/89 - 1s - loss: 0.3789 - accuracy: 0.9147\n",
            "Epoch 253/300\n",
            "89/89 - 1s - loss: 0.3770 - accuracy: 0.9137\n",
            "Epoch 254/300\n",
            "89/89 - 1s - loss: 0.3772 - accuracy: 0.9141\n",
            "Epoch 255/300\n",
            "89/89 - 1s - loss: 0.3892 - accuracy: 0.9089\n",
            "Epoch 256/300\n",
            "89/89 - 1s - loss: 0.3984 - accuracy: 0.9045\n",
            "Epoch 257/300\n",
            "89/89 - 1s - loss: 0.4153 - accuracy: 0.9002\n",
            "Epoch 258/300\n",
            "89/89 - 1s - loss: 0.4034 - accuracy: 0.9037\n",
            "Epoch 259/300\n",
            "89/89 - 1s - loss: 0.3943 - accuracy: 0.9021\n",
            "Epoch 260/300\n",
            "89/89 - 1s - loss: 0.3841 - accuracy: 0.9094\n",
            "Epoch 261/300\n",
            "89/89 - 1s - loss: 0.3519 - accuracy: 0.9200\n",
            "Epoch 262/300\n",
            "89/89 - 1s - loss: 0.3299 - accuracy: 0.9265\n",
            "Epoch 263/300\n",
            "89/89 - 1s - loss: 0.3248 - accuracy: 0.9258\n",
            "Epoch 264/300\n",
            "89/89 - 1s - loss: 0.3221 - accuracy: 0.9276\n",
            "Epoch 265/300\n",
            "89/89 - 1s - loss: 0.3149 - accuracy: 0.9294\n",
            "Epoch 266/300\n",
            "89/89 - 1s - loss: 0.3165 - accuracy: 0.9300\n",
            "Epoch 267/300\n",
            "89/89 - 1s - loss: 0.3236 - accuracy: 0.9278\n",
            "Epoch 268/300\n",
            "89/89 - 1s - loss: 0.3252 - accuracy: 0.9261\n",
            "Epoch 269/300\n",
            "89/89 - 1s - loss: 0.3106 - accuracy: 0.9309\n",
            "Epoch 270/300\n",
            "89/89 - 1s - loss: 0.3041 - accuracy: 0.9346\n",
            "Epoch 271/300\n",
            "89/89 - 1s - loss: 0.3013 - accuracy: 0.9338\n",
            "Epoch 272/300\n",
            "89/89 - 1s - loss: 0.3095 - accuracy: 0.9302\n",
            "Epoch 273/300\n",
            "89/89 - 1s - loss: 0.3133 - accuracy: 0.9298\n",
            "Epoch 274/300\n",
            "89/89 - 1s - loss: 0.3302 - accuracy: 0.9210\n",
            "Epoch 275/300\n",
            "89/89 - 1s - loss: 0.3165 - accuracy: 0.9276\n",
            "Epoch 276/300\n",
            "89/89 - 1s - loss: 0.3121 - accuracy: 0.9290\n",
            "Epoch 277/300\n",
            "89/89 - 1s - loss: 0.2990 - accuracy: 0.9341\n",
            "Epoch 278/300\n",
            "89/89 - 1s - loss: 0.2820 - accuracy: 0.9395\n",
            "Epoch 279/300\n",
            "89/89 - 1s - loss: 0.2817 - accuracy: 0.9395\n",
            "Epoch 280/300\n",
            "89/89 - 1s - loss: 0.2584 - accuracy: 0.9469\n",
            "Epoch 281/300\n",
            "89/89 - 1s - loss: 0.2339 - accuracy: 0.9531\n",
            "Epoch 282/300\n",
            "89/89 - 1s - loss: 0.2270 - accuracy: 0.9567\n",
            "Epoch 283/300\n",
            "89/89 - 1s - loss: 0.2087 - accuracy: 0.9625\n",
            "Epoch 284/300\n",
            "89/89 - 1s - loss: 0.2032 - accuracy: 0.9646\n",
            "Epoch 285/300\n",
            "89/89 - 1s - loss: 0.2002 - accuracy: 0.9637\n",
            "Epoch 286/300\n",
            "89/89 - 1s - loss: 0.2044 - accuracy: 0.9611\n",
            "Epoch 287/300\n",
            "89/89 - 1s - loss: 0.2151 - accuracy: 0.9591\n",
            "Epoch 288/300\n",
            "89/89 - 1s - loss: 0.2516 - accuracy: 0.9457\n",
            "Epoch 289/300\n",
            "89/89 - 1s - loss: 0.2915 - accuracy: 0.9327\n",
            "Epoch 290/300\n",
            "89/89 - 1s - loss: 0.4073 - accuracy: 0.8916\n",
            "Epoch 291/300\n",
            "89/89 - 1s - loss: 0.4969 - accuracy: 0.8553\n",
            "Epoch 292/300\n",
            "89/89 - 1s - loss: 0.4165 - accuracy: 0.8846\n",
            "Epoch 293/300\n",
            "89/89 - 1s - loss: 0.3400 - accuracy: 0.9141\n",
            "Epoch 294/300\n",
            "89/89 - 1s - loss: 0.2698 - accuracy: 0.9369\n",
            "Epoch 295/300\n",
            "89/89 - 1s - loss: 0.2181 - accuracy: 0.9539\n",
            "Epoch 296/300\n",
            "89/89 - 1s - loss: 0.1782 - accuracy: 0.9678\n",
            "Epoch 297/300\n",
            "89/89 - 1s - loss: 0.1533 - accuracy: 0.9750\n",
            "Epoch 298/300\n",
            "89/89 - 1s - loss: 0.1441 - accuracy: 0.9776\n",
            "Epoch 299/300\n",
            "89/89 - 1s - loss: 0.1391 - accuracy: 0.9789\n",
            "Epoch 300/300\n",
            "89/89 - 1s - loss: 0.1376 - accuracy: 0.9794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f727016dac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddyd_aiBkDKS",
        "colab_type": "text"
      },
      "source": [
        "Create a function to generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F7rse_xTnRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed Text is the input text which will be provided for generation\n",
        "# num_words tells how much word to be generated\n",
        "\n",
        "\n",
        "def generate_text(model,tokenizer, seq_length, seed_text,num_words):\n",
        "  text=[]\n",
        "\n",
        "  for _ in range (num_words):\n",
        "\n",
        "    #Convert the text into sequences\n",
        "    encoded_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "    # Add a padding upto seq length\n",
        "    padded_text= pad_sequences([encoded_text], maxlen=seq_length, truncating='pre')\n",
        "\n",
        "    # Model will predict the id of the word \n",
        "    pred_index=np.argmax(model.predict(padded_text), axis=-1)\n",
        "\n",
        "    # predictited word can be generated from tokenizer.word_index\n",
        "    pred_word =' '\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if index == pred_index:\n",
        "        pred_word = word\n",
        "        break\n",
        "    # Update the seed_text for next prediction\n",
        "    seed_text= seed_text + '  ' + pred_word\n",
        "\n",
        "    text.append(pred_word)\n",
        "\n",
        "  #Return  generated sentence\n",
        "  return ' '.join(text)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cGZ1j_-XncY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "10d1a2ac-e126-4f81-e925-35acb63f8b58"
      },
      "source": [
        "# Using randint to generate seed randomly\n",
        "from numpy.random import randint\n",
        "\n",
        "seq_len=xtrain.shape[1]\n",
        "seed= randint(0,len(tokens)-seq_len)\n",
        "\n",
        "# Create a random seed_text\n",
        "seed_text=' ' .join(xtrain_sentence[seed])\n",
        "\n",
        "print(' Seed Text are :' ,'\\n', seed_text,'\\n')\n",
        "print('Generated text are:',end='\\n')\n",
        "\n",
        "generate_text(model,tokenizer,seq_len,seed_text,10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Seed Text are : \n",
            " not being much accustomed to boots his pair of damp wrinkled cowhide ones probably not made to order either rather pinched and tormented him at the first go off of a \n",
            "\n",
            "Generated text are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitter cold morning seeing now that there were no curtains'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}